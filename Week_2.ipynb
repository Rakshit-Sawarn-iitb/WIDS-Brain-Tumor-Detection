{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakshit-Sawarn-iitb/WIDS-Brain-Tumor-Detection/blob/main/Week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable\n",
        "\n",
        "This week, your task involves conducting multi-class linear regression on batsmen salaries. You'll use the average runs scored per game and the strike rate as independent variables. The goal is to predict the salary as the dependent variable. Additionally, you'll be categorizing the data based on the years.\n",
        "\n",
        "The dataset is Data_Mendeley.csv given on GitHub. Feel free to create any new functions required."
      ],
      "metadata": {
        "id": "4OSeJ-R-y9s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import important libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "AZ77VEImzRW5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing data"
      ],
      "metadata": {
        "id": "2oQPpSttzqt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NgyvUuEMAVEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127057c0-7523-4d23-f645-1288e111d196"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Data_Mendeley.csv\")\n",
        "selected_columns = ['Ave', 'StrRate', 'Final Price']\n",
        "data = data[selected_columns]"
      ],
      "metadata": {
        "id": "C9C0MZOGzg7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (data == 0).any(axis=1)\n",
        "data = data[~mask]\n",
        "data.head()\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "-eIyaG4ATb7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns = 'Final Price')\n",
        "y = data['Final Price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
      ],
      "metadata": {
        "id": "jaGXv1Bxki6g"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward pass"
      ],
      "metadata": {
        "id": "LV9ROCrR1hQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  result = x*weights + bias\n",
        "  y_pred = np.sum(result, axis = 0)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "AgySD6Ac1DCw"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Loss"
      ],
      "metadata": {
        "id": "B3uHaq5x1i7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y,y_pred): #Mean Squared Loss\n",
        "  total_error = 0\n",
        "  samples = y.shape[0]\n",
        "  for i in range(samples):\n",
        "    total_error += (y-y_pred)**2\n",
        "  total_error = total_error/float(samples)\n",
        "  #loss = np.mean((y-y_pred)**2)\n",
        "  #return loss\n",
        "  return total_error"
      ],
      "metadata": {
        "id": "eP2rV0z21IaN"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Linear regression here :)"
      ],
      "metadata": {
        "id": "0JoKzzr_1uvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Linear_Regression(x,y,LearningRate,Iterations):\n",
        "  n_samples, n_features = x.shape\n",
        "  weights = np.zeros(n_features)\n",
        "  bias = 0\n",
        "\n",
        "  for i in range(Iterations):\n",
        "    ypred = np.dot(x, weights) + bias\n",
        "\n",
        "    dweights = (1/n_samples) * np.dot(x.T, (ypred-y))\n",
        "    dbias = (1/n_samples) * np.sum(ypred-y)\n",
        "\n",
        "    weights = weights - LearningRate * dweights\n",
        "    bias = bias - LearningRate * dbias\n",
        "\n",
        "  return weights, bias"
      ],
      "metadata": {
        "id": "k2W3q6eR1d2J"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x, weights, bias):\n",
        "  y_pred = np.dot(x, weights) + bias\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "junB88Ph726F"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LearningRate = 0.0001\n",
        "Iterations = 100000\n",
        "\n",
        "weights,bias = Linear_Regression(X_train, y_train, LearningRate, Iterations)\n",
        "\n",
        "print(weights,bias)\n",
        "\n",
        "pred = predict(X_test, weights, bias)\n",
        "#loss = loss(y_test,pred)\n",
        "#print(loss)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDhd15FH3nZs",
        "outputId": "0f63a664-b8e8-47d5-ca0c-6d0d672ee5c2"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 76026.08541052 364525.16136959] 2973.8469817073174\n",
            "[51116047.08711845 41740252.14733753 20632476.98811494 51703640.95256691\n",
            " 55381163.56344599 46742189.50795269 39451570.40127659 39344481.22954024\n",
            " 63240421.58240381 62560640.60162888 50114843.38103966 12380675.73166165\n",
            "  6668054.32338608 58937929.01691091 46081502.82330524 56737085.08272763\n",
            " 34651846.56071927 59215560.44739848 69712179.9104309  22102561.78538857\n",
            " 41217396.50228474 60352350.28007378 38178621.46253854 46067829.85976955\n",
            " 66414680.11791193 47812901.1259317  32062377.24664499 32697199.36363114\n",
            " 40937028.53715213 33898571.86958248 32433704.91935383 28986869.48822881\n",
            " 21988522.65727279 51568679.03659771 65471120.35503075 59695817.34689541\n",
            " 44905795.19773955 55506686.78260591 54483315.16015301 48290933.28337291\n",
            " 27684478.33404817 64528751.96182323 46155956.31328448 52624224.76830512\n",
            "  8152900.1489849  34747403.36595951 35664946.3067937  63317107.74800581\n",
            " 51797757.41433153 10569509.87813189 53879608.20321576 36987672.58181418\n",
            " 57499515.14391911 42269117.97158242 21531817.80860662 49571604.84523228\n",
            " 44371631.51672949 47638097.00148172 49616124.32960964 45274573.61719124\n",
            " 57669209.36704139 27120072.50890611 35721173.53870445 55681330.71910314\n",
            " 42830938.50539077 52038984.26710929 46253305.57217411 43902706.79091581\n",
            " 43060661.77023152 36759594.32558262 47765540.62243837 26252598.67021319\n",
            " 41064076.07629141 41255537.99867452 48049630.72468329 59674921.82098951\n",
            " 47531208.52809542 81034796.28780685 48448891.48593992 44621293.31505553\n",
            " 67369135.46226172 80674000.35455468 49210737.00433937 37841793.83287691\n",
            " 50120417.47355431 44023484.11927239 55447726.47729647 43625567.48761231\n",
            " 46610576.26839286 60315757.91543891 58552339.70257842 37025685.62451944\n",
            " 57723243.25413989 39223211.94251201 31863597.22206218 42968265.68596318\n",
            " 44886003.94078205 49973583.14977827 56373316.38394102 12456701.81707216\n",
            " 31402718.95652169 59847521.37432622 52929249.39024166 50602714.95794062\n",
            " 45920159.40071513 32720179.10904525 40480323.68848596 39174251.07610267\n",
            " 54910990.24786527 31927527.07466421 40937404.44802707 49409661.34974082\n",
            " 33851531.74199466 46686618.5579623  62061089.21319164 37253763.88075099\n",
            " 59972932.51247295 44502560.53837186 39154808.29956018 43141537.38835436\n",
            " 44070896.52797641 67629392.1315561  69844409.5955634  10569509.87813189\n",
            " 45076953.73355104 44147142.47160503 46712431.38892531 59028400.22706183\n",
            " 76304771.94434597 50984293.83054829 60260603.38672072 42269117.97158242\n",
            " 51437509.64877001 17053888.30041977 43324620.068671   18381284.08628217\n",
            " 54995787.52186371 59699186.36275961 18305258.00087165 53721377.73168278\n",
            " 57318228.88403539 51829143.832272   37386493.12109742 70154575.51607107\n",
            " 57602046.38028955 18305258.00087165 57678000.05252217 28619467.26966669\n",
            " 40837202.10115848 46896842.50021677 53183096.8428174  18305258.00087165\n",
            " 38336415.67888166 33494473.13893934 45501507.26258951 25220992.46353726\n",
            " 51188947.81558404 64120588.60258503 39558807.01805311 47170992.83382694\n",
            " 27261264.96782272 51396759.22885774 46272008.09029254 47364230.83718811\n",
            " 53101720.99543104]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Logistic regression is a process of modeling the probability of a discrete outcome given an input variable. The most common logistic regression models a binary outcome; something that can take two values such as true/false, yes/no, and so on.\n",
        "\n",
        "In this week you will be doing logistic regression on breast cancer dataset using sklearn library. Feel free to create any new functions required."
      ],
      "metadata": {
        "id": "aTAky_OS1w0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importinf libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "E56ck0_P2NR9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "V2sT9byQm1A2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data"
      ],
      "metadata": {
        "id": "qojSAol72cmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target"
      ],
      "metadata": {
        "id": "_uUSV8Xk2ePh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "N6jcbk5g29XW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward pass"
      ],
      "metadata": {
        "id": "ofghhz-63Ru5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_log(x):\n",
        "  y_pred = 1/(1 + np.exp(-x))\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "K1JzUVko3Zob"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary cross entropy loss"
      ],
      "metadata": {
        "id": "R4ldHUJs3d2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BCELoss(y,y_pred):\n",
        "  bce = ((y)*(np.log(y_pred)))+((1-y)*(np.log(1-y_pred)))\n",
        "  return bce"
      ],
      "metadata": {
        "id": "QkXgo04D3dGW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Logistic Regression here :)"
      ],
      "metadata": {
        "id": "OIuuOJcJ3sti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LogisticRegression(x,y,LearningRate,Iterations):\n",
        "  n_samples, n_features = x.shape\n",
        "  weights = np.zeros(n_features)\n",
        "  bias = 0\n",
        "\n",
        "  for i in range(Iterations):\n",
        "    linear_pred = np.dot(x,weights) + bias\n",
        "    predictions = forward_log(linear_pred)\n",
        "\n",
        "    dweights = (1/n_samples) * np.dot(x.T, (predictions - y))\n",
        "    dbias = (1/n_samples) * np.sum(predictions-y)\n",
        "\n",
        "    weights = weights - LearningRate*dweights\n",
        "    bias = bias - LearningRate*dbias\n",
        "\n",
        "  return weights,bias"
      ],
      "metadata": {
        "id": "gBJ6H_ss3yUr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x,weights,bias):\n",
        "  linearpred = np.dot(x, weights) + bias\n",
        "  ypred = forward_log(linearpred)\n",
        "  pred = [0 if y<=0.5 else 1 for y in ypred]\n",
        "\n",
        "  return pred"
      ],
      "metadata": {
        "id": "FnLEOTZ1uOYq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred,y_test):\n",
        "  acc = np.sum(y_pred==y_test)/len(y_test)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "WDlY7wx32Shy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LearningRate = 0.001\n",
        "Iterations = 1000000\n",
        "\n",
        "weights,bias = LogisticRegression(X_train,y_train,LearningRate,Iterations)\n",
        "\n",
        "pred = predict(X_test,weights,bias)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = accuracy(pred,y_test)*100\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsRw7-nUvJQD",
        "outputId": "431bb1f8-d85b-4650-b4ce-ba44cb169917"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96.49122807017544\n"
          ]
        }
      ]
    }
  ]
}